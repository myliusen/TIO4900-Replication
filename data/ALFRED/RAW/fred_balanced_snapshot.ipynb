{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83f742c0",
   "metadata": {},
   "source": [
    "# Auditable FRED/ALFRED Balanced Snapshot (125 series)\n",
    "This notebook builds a month-end balanced snapshot from the Fan et al. 125-series list with clear mapping, availability diagnostics, and source tracking.\n",
    "\n",
    "## Outputs\n",
    "- `snapshot_raw`: clean paper-mnemonic dataset (125 columns)\n",
    "- `snapshot_tcode`: transformed dataset via McCracken-Ng tcodes\n",
    "- `snapshot_obsdate`: last observation date used for each decision date/series\n",
    "- `snapshot_source`: source used per cell (`ALFRED`, `FRED_FALLBACK`, `COMPUTED_*`, missing)\n",
    "- Metadata/audit sheets: mapping, availability, coverage, revision diagnostics, issues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ddc237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "try:\n",
    "    from IPython.display import display\n",
    "except Exception:\n",
    "    def display(x):\n",
    "        print(x)\n",
    "\n",
    "# ---------- Config ----------\n",
    "API_KEY = os.getenv(\"FRED_API_KEY\") or \"3c268be920acd9693b77b400b0a95cf2\"\n",
    "BASE = \"https://api.stlouisfed.org/fred\"\n",
    "SERIES_CSV = \"data/fan_et_al_FREDMD_series.csv\"\n",
    "OUT_XLSX = \"data/fred_balanced_snapshot_auditable.xlsx\"\n",
    "START, END = \"1999-01-01\", \"2025-12-31\"\n",
    "STRICT_ALFRED = True          # True: never use revised FRED fallback when ALFRED is missing\n",
    "CHUNK_MONTHS, PAD_MONTHS = 24, 24\n",
    "PROBE_VINTAGES = \"2000-01-31,2000-02-29\"\n",
    "S = requests.Session()\n",
    "\n",
    "\n",
    "def month_ends(start, end):\n",
    "    try:\n",
    "        return pd.date_range(start, end, freq=\"ME\")\n",
    "    except Exception:\n",
    "        return pd.date_range(start, end, freq=\"M\")\n",
    "\n",
    "\n",
    "def jget(endpoint, **params):\n",
    "    params = dict(params, api_key=API_KEY, file_type=\"json\")\n",
    "    backoff = 0.5\n",
    "    for _ in range(8):\n",
    "        r = S.get(f\"{BASE}/{endpoint}\", params=params, timeout=30)\n",
    "        if r.status_code == 200:\n",
    "            return r.json()\n",
    "        if r.status_code in (429, 500, 502, 503, 504):\n",
    "            time.sleep(backoff)\n",
    "            backoff = min(backoff * 2, 20)\n",
    "            continue\n",
    "        raise RuntimeError(f\"{r.status_code}: {r.text[:320]}\")\n",
    "    raise RuntimeError(\"retry_exhausted\")\n",
    "\n",
    "\n",
    "def try_jget(endpoint, **params):\n",
    "    try:\n",
    "        return jget(endpoint, **params), \"\"\n",
    "    except RuntimeError as e:\n",
    "        return None, str(e)\n",
    "\n",
    "\n",
    "def month_start(ts):\n",
    "    return ts.to_period(\"M\").start_time.normalize()\n",
    "\n",
    "print(f\"API key loaded: {'yes' if API_KEY else 'no'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1383b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 1) Load and resolve mapping ----------\n",
    "a1 = pd.read_csv(SERIES_CSV)\n",
    "a1.columns = [c.strip() for c in a1.columns]\n",
    "a1[\"fred_raw\"] = a1[\"fred_raw\"].astype(str).str.strip()\n",
    "a1[\"tcode\"] = pd.to_numeric(a1[\"tcode\"], errors=\"raise\").astype(int)\n",
    "\n",
    "OVERRIDES = {\n",
    "    \"S&P500\": \"SP500\",\n",
    "    \"COMPAPFF\": \"CPFFM\",\n",
    "    \"COMPAPFFx\": \"CPFFM\",\n",
    "}\n",
    "CONSTRUCTED = {\n",
    "    \"CONSPI\": (\"NONREVSL\", \"PI\"),\n",
    "    \"HWIURATIO\": (\"HWI\", \"UNEMPLOY\"),\n",
    "}\n",
    "\n",
    "a1[\"series_id\"] = a1[\"fred_raw\"].replace(OVERRIDES)\n",
    "xmask = a1[\"series_id\"].str.endswith(\"x\", na=False)\n",
    "a1.loc[xmask, \"series_id\"] = a1.loc[xmask, \"series_id\"].str[:-1]\n",
    "a1[\"is_constructed\"] = a1[\"fred_raw\"].isin(CONSTRUCTED)\n",
    "a1.loc[a1[\"is_constructed\"], \"series_id\"] = pd.NA\n",
    "\n",
    "deps = sorted({d for ds in CONSTRUCTED.values() for d in ds if d != \"HWI\"})\n",
    "series_ids = list(dict.fromkeys(a1[\"series_id\"].dropna().tolist() + deps))\n",
    "\n",
    "# tcode map in both namespaces (paper and mapped API id)\n",
    "tcode_by_raw = dict(zip(a1[\"fred_raw\"], a1[\"tcode\"]))\n",
    "tcode_by_series = (\n",
    "    a1.dropna(subset=[\"series_id\"]).drop_duplicates(\"series_id\").set_index(\"series_id\")[\"tcode\"].to_dict()\n",
    ")\n",
    "\n",
    "mapping_cols = [c for c in [\"no\", \"fred_raw\", \"series_id\", \"tcode\", \"description\", \"group\", \"is_constructed\"] if c in a1.columns]\n",
    "mapping_df = a1[mapping_cols].copy()\n",
    "\n",
    "print(\"Rows in paper mapping:\", len(a1))\n",
    "print(\"Mapped API series ids:\", len(series_ids))\n",
    "print(\"Constructed series:\", a1.loc[a1[\"is_constructed\"], \"fred_raw\"].tolist())\n",
    "print(\"Override examples:\")\n",
    "display(mapping_df.loc[mapping_df[\"fred_raw\"] != mapping_df[\"series_id\"], [\"fred_raw\", \"series_id\"]].head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1156a399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 2) Availability diagnostics (FRED vs ALFRED) ----------\n",
    "def classify_availability(series_id):\n",
    "    _, e1 = try_jget(\"series\", series_id=series_id)\n",
    "    if e1:\n",
    "        if \"Invalid value for variable series_id\" in e1:\n",
    "            return \"INVALID_SERIES_ID\", e1\n",
    "        if \"does not exist\" in e1:\n",
    "            return \"NOT_IN_FRED\", e1\n",
    "        return \"SERIES_ERROR\", e1\n",
    "\n",
    "    _, e2 = try_jget(\n",
    "        \"series/observations\",\n",
    "        series_id=series_id,\n",
    "        output_type=2,\n",
    "        vintage_dates=PROBE_VINTAGES,\n",
    "        observation_start=\"1999-01-01\",\n",
    "        observation_end=\"2000-02-29\",\n",
    "        sort_order=\"asc\",\n",
    "        limit=100000,\n",
    "    )\n",
    "    if not e2:\n",
    "        return \"ALFRED_OK\", \"\"\n",
    "    if \"does not exist in ALFRED\" in e2:\n",
    "        return \"FRED_ONLY_NO_ALFRED\", e2\n",
    "    return \"OBS_ERROR\", e2\n",
    "\n",
    "\n",
    "rows = []\n",
    "for i, sid in enumerate(series_ids, 1):\n",
    "    status, detail = classify_availability(sid)\n",
    "    rows.append((sid, status, detail))\n",
    "    if i % 25 == 0:\n",
    "        print(f\"availability progress: {i}/{len(series_ids)}\")\n",
    "    time.sleep(0.05)\n",
    "\n",
    "availability_df = pd.DataFrame(rows, columns=[\"series_id\", \"availability\", \"detail\"])\n",
    "\n",
    "# Attach availability to each paper mnemonic row\n",
    "mapping_audit_df = mapping_df.merge(availability_df, how=\"left\", on=\"series_id\")\n",
    "print(\"Availability counts:\")\n",
    "print(availability_df[\"availability\"].value_counts())\n",
    "display(mapping_audit_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137f077b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 3) Download balanced snapshot with source tracking ----------\n",
    "idx = pd.to_datetime(month_ends(START, END))\n",
    "X = pd.DataFrame(index=idx, columns=series_ids, dtype=\"float64\")\n",
    "X_obs = pd.DataFrame(index=idx, columns=series_ids, dtype=\"datetime64[ns]\")\n",
    "X_src = pd.DataFrame(index=idx, columns=series_ids, dtype=\"object\")\n",
    "issues = []\n",
    "\n",
    "\n",
    "def fetch_series_chunk(sid, vints, obs_start, obs_end):\n",
    "    js, err = try_jget(\n",
    "        \"series/observations\",\n",
    "        series_id=sid,\n",
    "        output_type=2,\n",
    "        vintage_dates=\",\".join(vints),\n",
    "        observation_start=obs_start,\n",
    "        observation_end=obs_end,\n",
    "        sort_order=\"asc\",\n",
    "        limit=100000,\n",
    "    )\n",
    "    if err:\n",
    "        if \"does not exist in ALFRED\" in err:\n",
    "            if STRICT_ALFRED:\n",
    "                return {}, {}, {}, \"MISSING_ALFRED\"\n",
    "            js, err2 = try_jget(\n",
    "                \"series/observations\",\n",
    "                series_id=sid,\n",
    "                observation_start=obs_start,\n",
    "                observation_end=obs_end,\n",
    "                sort_order=\"asc\",\n",
    "                limit=100000,\n",
    "            )\n",
    "            if err2:\n",
    "                return {}, {}, {}, f\"FRED_ERROR: {err2[:120]}\"\n",
    "            df = pd.DataFrame(js.get(\"observations\", []) or [])\n",
    "            if not {\"date\", \"value\"} <= set(df.columns):\n",
    "                return {}, {}, {}, \"FRED_BAD_SHAPE\"\n",
    "            df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "            df[\"value\"] = pd.to_numeric(df[\"value\"].replace(\".\", np.nan), errors=\"coerce\")\n",
    "            df = df.dropna(subset=[\"date\", \"value\"]).sort_values(\"date\")\n",
    "            vals, obs, src = {}, {}, {}\n",
    "            for v in vints:\n",
    "                d = pd.to_datetime(v)\n",
    "                g = df[df[\"date\"] <= d]\n",
    "                if g.empty:\n",
    "                    continue\n",
    "                last = g.iloc[-1]\n",
    "                vals[d], obs[d], src[d] = float(last[\"value\"]), last[\"date\"], \"FRED_FALLBACK\"\n",
    "            return vals, obs, src, \"\"\n",
    "        return {}, {}, {}, f\"ALFRED_ERROR: {err[:120]}\"\n",
    "\n",
    "    # ALFRED output_type=2 => date + one column per vintage\n",
    "    df = pd.DataFrame(js.get(\"observations\", []) or [])\n",
    "    if \"date\" not in df.columns:\n",
    "        return {}, {}, {}, \"ALFRED_BAD_SHAPE\"\n",
    "\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    vals, obs, src = {}, {}, {}\n",
    "    for v in vints:\n",
    "        d = pd.to_datetime(v)\n",
    "        col = f\"{sid}_{d.strftime('%Y%m%d')}\"\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        g = pd.DataFrame({\n",
    "            \"date\": df[\"date\"],\n",
    "            \"value\": pd.to_numeric(df[col].replace(\".\", np.nan), errors=\"coerce\"),\n",
    "        }).dropna(subset=[\"date\", \"value\"])\n",
    "        if g.empty:\n",
    "            continue\n",
    "        last = g.iloc[-1]\n",
    "        vals[d], obs[d], src[d] = float(last[\"value\"]), last[\"date\"], \"ALFRED\"\n",
    "    return vals, obs, src, \"\"\n",
    "\n",
    "\n",
    "for i0 in range(0, len(idx), CHUNK_MONTHS):\n",
    "    chunk = idx[i0:i0 + CHUNK_MONTHS]\n",
    "    vints = [d.strftime(\"%Y-%m-%d\") for d in chunk]\n",
    "    obs_start = (month_start(chunk[0]) - pd.DateOffset(months=PAD_MONTHS)).strftime(\"%Y-%m-%d\")\n",
    "    obs_end = chunk[-1].strftime(\"%Y-%m-%d\")\n",
    "    print(f\"chunk {i0 // CHUNK_MONTHS + 1}: {vints[0]}..{vints[-1]}\")\n",
    "\n",
    "    for sid in series_ids:\n",
    "        vals, obsd, srcd, issue = fetch_series_chunk(sid, vints, obs_start, obs_end)\n",
    "        if issue:\n",
    "            issues.append({\"series_id\": sid, \"stage\": \"fetch\", \"issue\": issue})\n",
    "        for d, v in vals.items():\n",
    "            X.loc[d, sid] = v\n",
    "            X_obs.loc[d, sid] = obsd[d]\n",
    "            X_src.loc[d, sid] = srcd[d]\n",
    "\n",
    "print(\"Built API-level matrices:\", X.shape)\n",
    "print(\"Cell source counts:\")\n",
    "print(X_src.stack().value_counts(dropna=False).head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2394d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 4) Build paper-level dataset (125 columns) ----------\n",
    "X_paper = pd.DataFrame(index=idx)\n",
    "X_paper_obs = pd.DataFrame(index=idx)\n",
    "X_paper_src = pd.DataFrame(index=idx)\n",
    "\n",
    "for _, r in a1.iterrows():\n",
    "    raw, sid = r[\"fred_raw\"], r[\"series_id\"]\n",
    "    if pd.notna(sid) and sid in X.columns:\n",
    "        X_paper[raw] = X[sid]\n",
    "        X_paper_obs[raw] = X_obs[sid]\n",
    "        X_paper_src[raw] = X_src[sid]\n",
    "    else:\n",
    "        X_paper[raw] = np.nan\n",
    "        X_paper_obs[raw] = pd.NaT\n",
    "        X_paper_src[raw] = \"MISSING_MAPPING\"\n",
    "\n",
    "# Constructed features (when dependencies exist)\n",
    "if {\"NONREVSL\", \"PI\"} <= set(X.columns):\n",
    "    X_paper[\"CONSPI\"] = X[\"NONREVSL\"] / X[\"PI\"]\n",
    "    X_paper_obs[\"CONSPI\"] = pd.concat([X_obs[\"NONREVSL\"], X_obs[\"PI\"]], axis=1).max(axis=1)\n",
    "    same = X_src[\"NONREVSL\"].eq(X_src[\"PI\"])\n",
    "    X_paper_src[\"CONSPI\"] = np.where(same, \"COMPUTED_\" + X_src[\"NONREVSL\"].fillna(\"MISSING\"), \"COMPUTED_MIXED\")\n",
    "else:\n",
    "    issues.append({\"series_id\": \"CONSPI\", \"stage\": \"construct\", \"issue\": \"missing NONREVSL or PI\"})\n",
    "\n",
    "if {\"HWI\", \"UNEMPLOY\"} <= set(X.columns):\n",
    "    X_paper[\"HWIURATIO\"] = X[\"HWI\"] / X[\"UNEMPLOY\"]\n",
    "    X_paper_obs[\"HWIURATIO\"] = pd.concat([X_obs[\"HWI\"], X_obs[\"UNEMPLOY\"]], axis=1).max(axis=1)\n",
    "    same = X_src[\"HWI\"].eq(X_src[\"UNEMPLOY\"])\n",
    "    X_paper_src[\"HWIURATIO\"] = np.where(same, \"COMPUTED_\" + X_src[\"HWI\"].fillna(\"MISSING\"), \"COMPUTED_MIXED\")\n",
    "else:\n",
    "    issues.append({\"series_id\": \"HWIURATIO\", \"stage\": \"construct\", \"issue\": \"missing HWI or UNEMPLOY\"})\n",
    "\n",
    "\n",
    "# Diagnostics\n",
    "coverage_by_series = pd.DataFrame({\n",
    "    \"non_null_count\": X_paper.notna().sum(),\n",
    "    \"coverage_share\": X_paper.notna().mean(),\n",
    "}).sort_values([\"coverage_share\", \"non_null_count\"])\n",
    "coverage_by_date = pd.DataFrame({\"coverage_share\": X_paper.notna().mean(axis=1)})\n",
    "\n",
    "\n",
    "def rev_new_counts(v, d):\n",
    "    rev = d.eq(d.shift(1)) & v.ne(v.shift(1))\n",
    "    new = d.gt(d.shift(1))\n",
    "    return int(rev.sum()), int(new.sum())\n",
    "\n",
    "rev_new_diag = pd.DataFrame(\n",
    "    [rev_new_counts(X_paper[c], X_paper_obs[c]) for c in X_paper.columns],\n",
    "    index=X_paper.columns,\n",
    "    columns=[\"revision_events\", \"new_obs_events\"],\n",
    ")\n",
    "\n",
    "source_by_series = X_paper_src.apply(lambda s: s.value_counts(dropna=False)).fillna(0).astype(int)\n",
    "\n",
    "# Add high-impact edge cases to issue table\n",
    "for sid in availability_df.loc[availability_df[\"availability\"].isin([\"INVALID_SERIES_ID\", \"NOT_IN_FRED\", \"FRED_ONLY_NO_ALFRED\"]), \"series_id\"]:\n",
    "    issues.append({\"series_id\": sid, \"stage\": \"availability\", \"issue\": availability_df.set_index(\"series_id\").loc[sid, \"availability\"]})\n",
    "for sid in coverage_by_series.index[coverage_by_series[\"coverage_share\"] < 0.2]:\n",
    "    issues.append({\"series_id\": sid, \"stage\": \"coverage\", \"issue\": \"coverage_share < 0.20\"})\n",
    "\n",
    "issues_df = pd.DataFrame(issues).drop_duplicates().sort_values([\"stage\", \"series_id\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"Paper-level shape:\", X_paper.shape)\n",
    "print(\"Lowest coverage series:\")\n",
    "display(coverage_by_series.head(12))\n",
    "print(\"Issue count:\", len(issues_df))\n",
    "display(issues_df.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c45212b",
   "metadata": {},
   "source": [
    "## Edge Cases / Weaknesses to Watch\n",
    "- `FRED_ONLY_NO_ALFRED`: series exists in FRED but has no historical vintages, so strict non-revised snapshots are structurally sparse.\n",
    "- `INVALID_SERIES_ID`: legacy paper names that are not valid API IDs must be mapped manually.\n",
    "- Constructed series (`CONSPI`, `HWIURATIO`) depend on component availability and can be partially missing.\n",
    "- Even for `ALFRED_OK` series, some decision dates can miss a vintage column/value and reduce coverage.\n",
    "- If `STRICT_ALFRED=False`, revised values enter through `FRED_FALLBACK` and must be filtered in downstream analysis when non-revised purity is required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87d69ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 5) Transform + write auditable workbook ----------\n",
    "def apply_tcodes(df, tcode_map):\n",
    "    out = pd.DataFrame(index=df.index)\n",
    "    for col, x in df.items():\n",
    "        x = pd.to_numeric(x, errors=\"coerce\")\n",
    "        t = int(tcode_map.get(col, 1))\n",
    "        if t == 1: y = x\n",
    "        elif t == 2: y = x.diff()\n",
    "        elif t == 3: y = x.diff().diff()\n",
    "        elif t == 4: y = np.log(x.where(x > 0))\n",
    "        elif t == 5: y = np.log(x.where(x > 0)).diff()\n",
    "        elif t == 6: y = np.log(x.where(x > 0)).diff().diff()\n",
    "        elif t == 7: y = (x / x.shift(1) - 1.0).diff()\n",
    "        else:\n",
    "            y = np.nan\n",
    "            issues.append({\"series_id\": col, \"stage\": \"tcode\", \"issue\": f\"unknown tcode {t}\"})\n",
    "        out[col] = y\n",
    "    return out\n",
    "\n",
    "\n",
    "X_t = apply_tcodes(X_paper, tcode_by_raw)\n",
    "\n",
    "run_meta = pd.DataFrame([\n",
    "    (\"run_utc\", pd.Timestamp.utcnow().isoformat()),\n",
    "    (\"branch\", subprocess.check_output([\"git\", \"branch\", \"--show-current\"], text=True).strip()),\n",
    "    (\"series_csv\", SERIES_CSV),\n",
    "    (\"start\", START),\n",
    "    (\"end\", END),\n",
    "    (\"strict_alfred\", STRICT_ALFRED),\n",
    "    (\"chunk_months\", CHUNK_MONTHS),\n",
    "    (\"pad_months\", PAD_MONTHS),\n",
    "    (\"probe_vintages\", PROBE_VINTAGES),\n",
    "], columns=[\"key\", \"value\"])\n",
    "\n",
    "source_legend = pd.DataFrame([\n",
    "    (\"ALFRED\", \"Non-revised real-time vintage value\"),\n",
    "    (\"FRED_FALLBACK\", \"Revised/latest fallback (only when STRICT_ALFRED=False)\"),\n",
    "    (\"COMPUTED_*\", \"Constructed series from dependencies\"),\n",
    "    (\"MISSING_MAPPING\", \"No mapped source series id\"),\n",
    "], columns=[\"source\", \"meaning\"])\n",
    "\n",
    "with pd.ExcelWriter(OUT_XLSX, engine=\"openpyxl\") as w:\n",
    "    run_meta.to_excel(w, sheet_name=\"run_metadata\", index=False)\n",
    "    source_legend.to_excel(w, sheet_name=\"source_legend\", index=False)\n",
    "    mapping_df.to_excel(w, sheet_name=\"series_mapping\", index=False)\n",
    "    availability_df.to_excel(w, sheet_name=\"series_availability\", index=False)\n",
    "    mapping_audit_df.to_excel(w, sheet_name=\"mapping_audit\", index=False)\n",
    "    issues_df.to_excel(w, sheet_name=\"issues\", index=False)\n",
    "\n",
    "    X_paper.reset_index(names=\"decision_date\").to_excel(w, sheet_name=\"snapshot_raw\", index=False)\n",
    "    X_paper_obs.reset_index(names=\"decision_date\").to_excel(w, sheet_name=\"snapshot_obsdate\", index=False)\n",
    "    X_paper_src.reset_index(names=\"decision_date\").to_excel(w, sheet_name=\"snapshot_source\", index=False)\n",
    "    X_t.reset_index(names=\"decision_date\").to_excel(w, sheet_name=\"snapshot_tcode\", index=False)\n",
    "\n",
    "    coverage_by_series.reset_index(names=\"series_id\").to_excel(w, sheet_name=\"coverage_by_series\", index=False)\n",
    "    coverage_by_date.reset_index(names=\"decision_date\").to_excel(w, sheet_name=\"coverage_by_date\", index=False)\n",
    "    rev_new_diag.reset_index(names=\"series_id\").to_excel(w, sheet_name=\"rev_new_diag\", index=False)\n",
    "    source_by_series.reset_index(names=\"series_id\").to_excel(w, sheet_name=\"source_by_series\", index=False)\n",
    "\n",
    "print(\"Wrote:\", OUT_XLSX)\n",
    "print(\"Final shape raw/tcode:\", X_paper.shape, X_t.shape)\n",
    "print(\"Availability summary:\")\n",
    "print(availability_df[\"availability\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68705ecc",
   "metadata": {},
   "source": [
    "## 6) Post-process Existing Workbook: Latest-Vintage Backfill\n",
    "This section does **not** rerun the full ALFRED snapshot pull. It reads the existing Excel output and fills missing cells by:\n",
    "- using the **latest ALFRED vintage** when vintages exist,\n",
    "- otherwise using **latest FRED** for `FRED_ONLY_NO_ALFRED` series.\n",
    "\n",
    "All filled cells are source-labeled and diagnostics are written to new sheets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4982fa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "API_KEY = os.getenv(\"FRED_API_KEY\") or \"3c268be920acd9693b77b400b0a95cf2\"\n",
    "BASE = \"https://api.stlouisfed.org/fred\"\n",
    "IN_XLSX = \"data/fred_balanced_snapshot_auditable.xlsx\"\n",
    "OUT_XLSX = \"data/fred_balanced_snapshot_auditable_latest_vintage_backfill.xlsx\"\n",
    "WRITE_IN_PLACE = False  # set True to overwrite IN_XLSX\n",
    "S = requests.Session()\n",
    "\n",
    "\n",
    "def jget(endpoint, **params):\n",
    "    params = dict(params, api_key=API_KEY, file_type=\"json\")\n",
    "    backoff = 0.5\n",
    "    for _ in range(8):\n",
    "        r = S.get(f\"{BASE}/{endpoint}\", params=params, timeout=30)\n",
    "        if r.status_code == 200:\n",
    "            return r.json()\n",
    "        if r.status_code in (429, 500, 502, 503, 504):\n",
    "            time.sleep(backoff)\n",
    "            backoff = min(backoff * 2, 20)\n",
    "            continue\n",
    "        raise RuntimeError(f\"{r.status_code}: {r.text[:320]}\")\n",
    "    raise RuntimeError(\"retry_exhausted\")\n",
    "\n",
    "\n",
    "def try_jget(endpoint, **params):\n",
    "    try:\n",
    "        return jget(endpoint, **params), \"\"\n",
    "    except RuntimeError as e:\n",
    "        return None, str(e)\n",
    "\n",
    "\n",
    "def get_vintages(series_id):\n",
    "    js, err = try_jget(\"series/vintagedates\", series_id=series_id, limit=10000)\n",
    "    if err:\n",
    "        return [], err\n",
    "    return js.get(\"vintage_dates\", []) or [], \"\"\n",
    "\n",
    "\n",
    "def get_history(series_id, obs_start, obs_end, realtime_date=None):\n",
    "    p = dict(\n",
    "        series_id=series_id,\n",
    "        observation_start=obs_start,\n",
    "        observation_end=obs_end,\n",
    "        sort_order=\"asc\",\n",
    "        limit=100000,\n",
    "    )\n",
    "    if realtime_date is not None:\n",
    "        p[\"realtime_start\"] = realtime_date\n",
    "        p[\"realtime_end\"] = realtime_date\n",
    "\n",
    "    js, err = try_jget(\"series/observations\", **p)\n",
    "    if err:\n",
    "        return pd.DataFrame(columns=[\"date\", \"value\"]), err\n",
    "\n",
    "    df = pd.DataFrame(js.get(\"observations\", []) or [])\n",
    "    if not {\"date\", \"value\"} <= set(df.columns):\n",
    "        return pd.DataFrame(columns=[\"date\", \"value\"]), \"bad_shape\"\n",
    "\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    df[\"value\"] = pd.to_numeric(df[\"value\"].replace(\".\", np.nan), errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"date\", \"value\"]).sort_values(\"date\")\n",
    "    return df[[\"date\", \"value\"]], \"\"\n",
    "\n",
    "\n",
    "# Load base workbook outputs\n",
    "base_xls = pd.ExcelFile(IN_XLSX)\n",
    "all_sheets = {s: pd.read_excel(IN_XLSX, sheet_name=s) for s in base_xls.sheet_names}\n",
    "\n",
    "raw_b = all_sheets[\"snapshot_raw\"].copy()\n",
    "obs_b = all_sheets[\"snapshot_obsdate\"].copy()\n",
    "src_b = all_sheets[\"snapshot_source\"].copy()\n",
    "# source labels may load as float64 when mostly NaN; force object before writing strings\n",
    "src_cols = [c for c in src_b.columns if c != \"decision_date\"]\n",
    "src_b[src_cols] = src_b[src_cols].astype(\"object\")\n",
    "\n",
    "map_df = all_sheets[\"series_mapping\"].copy()\n",
    "avail_df = all_sheets.get(\"series_availability\", pd.DataFrame(columns=[\"series_id\", \"availability\"]))\n",
    "\n",
    "raw_before = raw_b.copy()\n",
    "obs_before = obs_b.copy()\n",
    "src_before = src_b.copy()\n",
    "\n",
    "decision = pd.to_datetime(raw_b[\"decision_date\"])\n",
    "obs_start = (decision.min() - pd.DateOffset(years=2)).strftime(\"%Y-%m-%d\")\n",
    "obs_end = decision.max().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "series_cols = [c for c in raw_b.columns if c != \"decision_date\"]\n",
    "raw_to_sid = map_df.drop_duplicates(\"fred_raw\").set_index(\"fred_raw\")[\"series_id\"].to_dict()\n",
    "avail_map = avail_df.drop_duplicates(\"series_id\").set_index(\"series_id\")[\"availability\"].to_dict() if len(avail_df) else {}\n",
    "\n",
    "for c in series_cols:\n",
    "    obs_b[c] = pd.to_datetime(obs_b[c], errors=\"coerce\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b9a615a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backfill progress: 25/125\n",
      "backfill progress: 50/125\n",
      "backfill progress: 75/125\n",
      "backfill progress: 100/125\n",
      "backfill progress: 125/125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dd/k7jhctjs7zxf5_58m9_zy7m40000gn/T/ipykernel_65994/530601157.py:120: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[col] = y\n",
      "/var/folders/dd/k7jhctjs7zxf5_58m9_zy7m40000gn/T/ipykernel_65994/530601157.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  Xb_t.insert(0, \"decision_date\", decision)\n",
      "/var/folders/dd/k7jhctjs7zxf5_58m9_zy7m40000gn/T/ipykernel_65994/530601157.py:132: Pandas4Warning: Timestamp.utcnow is deprecated and will be removed in a future version. Use Timestamp.now('UTC') instead.\n",
      "  (\"run_utc\", pd.Timestamp.utcnow().isoformat()),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote backfill workbook: fred_balanced_snapshot_auditable_latest_vintage_backfill.xlsx\n",
      "Rows x cols: (744, 126)\n",
      "Filled cells total: 24759\n",
      "Still missing total: 9060\n",
      "Top methods:\n",
      "            fill_method  series_count  total_filled_cells  total_still_missing  avg_vintage_count\n",
      "   BACKFILL_LATEST_FRED            36               17443                 2331                NaN\n",
      "BACKFILL_LATEST_VINTAGE            21                7316                 1095         472.952381\n",
      "                   NONE            60                   0                    0                NaN\n",
      " SKIP_INVALID_SERIES_ID             3                   0                 2232                NaN\n",
      "       SKIP_NOT_IN_FRED             3                   0                 2232                NaN\n",
      "        SKIP_NO_MAPPING             2                   0                 1170                NaN\n"
     ]
    }
   ],
   "source": [
    "diag_rows = []\n",
    "\n",
    "for i, raw_col in enumerate(series_cols, 1):\n",
    "    sid = raw_to_sid.get(raw_col)\n",
    "    availability = avail_map.get(sid, \"UNKNOWN\")\n",
    "\n",
    "    x_before = pd.to_numeric(raw_b[raw_col], errors=\"coerce\")\n",
    "    miss_before = x_before.isna()\n",
    "    filled_cells = 0\n",
    "    fill_method = \"NONE\"\n",
    "    ref_date = pd.NaT\n",
    "    first_vintage = pd.NaT\n",
    "    last_vintage = pd.NaT\n",
    "    vintage_count = np.nan\n",
    "    hist_obs_count = 0\n",
    "    err = \"\"\n",
    "\n",
    "    if miss_before.any():\n",
    "        if not isinstance(sid, str) or not sid.strip():\n",
    "            fill_method = \"SKIP_NO_MAPPING\"\n",
    "        elif availability in {\"INVALID_SERIES_ID\", \"NOT_IN_FRED\"}:\n",
    "            fill_method = f\"SKIP_{availability}\"\n",
    "        else:\n",
    "            use_realtime = None\n",
    "            if availability == \"FRED_ONLY_NO_ALFRED\":\n",
    "                fill_method = \"BACKFILL_LATEST_FRED\"\n",
    "            else:\n",
    "                vintages, v_err = get_vintages(sid)\n",
    "                if vintages:\n",
    "                    fill_method = \"BACKFILL_LATEST_VINTAGE\"\n",
    "                    vintage_count = len(vintages)\n",
    "                    first_vintage = pd.to_datetime(vintages[0])\n",
    "                    last_vintage = pd.to_datetime(vintages[-1])\n",
    "                    ref_date = last_vintage\n",
    "                    use_realtime = vintages[-1]\n",
    "                else:\n",
    "                    fill_method = \"BACKFILL_LATEST_FRED_IF_NO_VINTAGE\"\n",
    "                    err = v_err\n",
    "\n",
    "            hist, h_err = get_history(sid, obs_start, obs_end, realtime_date=use_realtime)\n",
    "            hist_obs_count = len(hist)\n",
    "            if h_err:\n",
    "                err = (err + \" | \" + h_err).strip(\" |\") if err else h_err\n",
    "            elif len(hist):\n",
    "                asof = pd.merge_asof(\n",
    "                    pd.DataFrame({\"decision_date\": decision}),\n",
    "                    hist.rename(columns={\"date\": \"obs_date\"}),\n",
    "                    left_on=\"decision_date\",\n",
    "                    right_on=\"obs_date\",\n",
    "                    direction=\"backward\",\n",
    "                )\n",
    "                can_fill = miss_before & asof[\"value\"].notna()\n",
    "                filled_cells = int(can_fill.sum())\n",
    "                if filled_cells:\n",
    "                    raw_b.loc[can_fill, raw_col] = asof.loc[can_fill, \"value\"].values\n",
    "                    obs_b.loc[can_fill, raw_col] = asof.loc[can_fill, \"obs_date\"].values\n",
    "                    src_b.loc[can_fill, raw_col] = fill_method\n",
    "\n",
    "        time.sleep(0.03)\n",
    "\n",
    "    x_after = pd.to_numeric(raw_b[raw_col], errors=\"coerce\")\n",
    "    miss_after = x_after.isna()\n",
    "    filled_idx = miss_before & x_after.notna()\n",
    "\n",
    "    diag_rows.append({\n",
    "        \"fred_raw\": raw_col,\n",
    "        \"series_id\": sid,\n",
    "        \"availability\": availability,\n",
    "        \"fill_method\": fill_method,\n",
    "        \"reference_realtime_date\": ref_date,\n",
    "        \"vintage_count\": vintage_count,\n",
    "        \"first_vintage\": first_vintage,\n",
    "        \"last_vintage\": last_vintage,\n",
    "        \"hist_obs_count\": hist_obs_count,\n",
    "        \"non_null_before\": int(x_before.notna().sum()),\n",
    "        \"non_null_after\": int(x_after.notna().sum()),\n",
    "        \"filled_cells\": filled_cells,\n",
    "        \"still_missing_after\": int(miss_after.sum()),\n",
    "        \"first_decision_with_data_before\": decision[x_before.notna()].min() if x_before.notna().any() else pd.NaT,\n",
    "        \"first_decision_with_data_after\": decision[x_after.notna()].min() if x_after.notna().any() else pd.NaT,\n",
    "        \"first_filled_decision_date\": decision[filled_idx].min() if filled_idx.any() else pd.NaT,\n",
    "        \"last_filled_decision_date\": decision[filled_idx].max() if filled_idx.any() else pd.NaT,\n",
    "        \"error\": err,\n",
    "    })\n",
    "\n",
    "    if i % 25 == 0:\n",
    "        print(f\"backfill progress: {i}/{len(series_cols)}\")\n",
    "\n",
    "\n",
    "diag_df = pd.DataFrame(diag_rows).sort_values([\"filled_cells\", \"still_missing_after\"], ascending=[False, False])\n",
    "\n",
    "summary_df = (\n",
    "    diag_df.groupby(\"fill_method\", dropna=False)\n",
    "    .agg(\n",
    "        series_count=(\"fred_raw\", \"count\"),\n",
    "        total_filled_cells=(\"filled_cells\", \"sum\"),\n",
    "        total_still_missing=(\"still_missing_after\", \"sum\"),\n",
    "        avg_vintage_count=(\"vintage_count\", \"mean\"),\n",
    "    )\n",
    "    .reset_index()\n",
    "    .sort_values(\"total_filled_cells\", ascending=False)\n",
    ")\n",
    "\n",
    "# transformed output for backfilled raw\n",
    "tcode_by_raw = map_df.drop_duplicates(\"fred_raw\").set_index(\"fred_raw\")[\"tcode\"].to_dict()\n",
    "\n",
    "def apply_tcodes(df, tcodes):\n",
    "    out = pd.DataFrame(index=df.index)\n",
    "    for col, x in df.items():\n",
    "        x = pd.to_numeric(x, errors=\"coerce\")\n",
    "        t = int(tcodes.get(col, 1))\n",
    "        if t == 1: y = x\n",
    "        elif t == 2: y = x.diff()\n",
    "        elif t == 3: y = x.diff().diff()\n",
    "        elif t == 4: y = np.log(x.where(x > 0))\n",
    "        elif t == 5: y = np.log(x.where(x > 0)).diff()\n",
    "        elif t == 6: y = np.log(x.where(x > 0)).diff().diff()\n",
    "        elif t == 7: y = (x / x.shift(1) - 1.0).diff()\n",
    "        else: y = np.nan\n",
    "        out[col] = y\n",
    "    return out\n",
    "\n",
    "Xb_t = apply_tcodes(raw_b.drop(columns=[\"decision_date\"]), tcode_by_raw)\n",
    "Xb_t.insert(0, \"decision_date\", decision)\n",
    "\n",
    "run_meta = pd.DataFrame([\n",
    "    (\"input_workbook\", IN_XLSX),\n",
    "    (\"output_workbook\", IN_XLSX if WRITE_IN_PLACE else OUT_XLSX),\n",
    "    (\"strategy\", \"latest_vintage_else_latest_fred\"),\n",
    "    (\"api_obs_start\", obs_start),\n",
    "    (\"api_obs_end\", obs_end),\n",
    "    (\"run_utc\", pd.Timestamp.utcnow().isoformat()),\n",
    "], columns=[\"key\", \"value\"])\n",
    "\n",
    "source_legend_new = pd.DataFrame([\n",
    "    (\"BACKFILL_LATEST_VINTAGE\", \"Filled missing cells using historical path at latest ALFRED vintage\"),\n",
    "    (\"BACKFILL_LATEST_FRED\", \"Filled missing cells using latest revised FRED path (no ALFRED history)\"),\n",
    "    (\"BACKFILL_LATEST_FRED_IF_NO_VINTAGE\", \"Fallback when ALFRED vintage listing failed/empty\"),\n",
    "], columns=[\"source\", \"meaning\"])\n",
    "\n",
    "out_path = IN_XLSX if WRITE_IN_PLACE else OUT_XLSX\n",
    "with pd.ExcelWriter(out_path, engine=\"openpyxl\") as w:\n",
    "    # keep original sheets\n",
    "    for name, df in all_sheets.items():\n",
    "        df.to_excel(w, sheet_name=name[:31], index=False)\n",
    "\n",
    "    # add backfilled datasets and diagnostics\n",
    "    raw_b.to_excel(w, sheet_name=\"snapshot_raw_bfill_latest\", index=False)\n",
    "    obs_b.to_excel(w, sheet_name=\"snapshot_obs_bfill_latest\", index=False)\n",
    "    src_b.to_excel(w, sheet_name=\"snapshot_src_bfill_latest\", index=False)\n",
    "    Xb_t.to_excel(w, sheet_name=\"snapshot_tcode_bfill_latest\", index=False)\n",
    "\n",
    "    diag_df.to_excel(w, sheet_name=\"bfill_diag_by_series\", index=False)\n",
    "    summary_df.to_excel(w, sheet_name=\"bfill_summary\", index=False)\n",
    "    diag_df[[\n",
    "        \"fred_raw\", \"series_id\", \"availability\", \"vintage_count\", \"first_vintage\", \"last_vintage\",\n",
    "        \"fill_method\", \"reference_realtime_date\", \"hist_obs_count\"\n",
    "    ]].to_excel(w, sheet_name=\"vintage_diag\", index=False)\n",
    "    run_meta.to_excel(w, sheet_name=\"bfill_run_meta\", index=False)\n",
    "    source_legend_new.to_excel(w, sheet_name=\"bfill_source_legend\", index=False)\n",
    "\n",
    "print(\"Wrote backfill workbook:\", out_path)\n",
    "print(\"Rows x cols:\", raw_b.shape)\n",
    "print(\"Filled cells total:\", int(diag_df[\"filled_cells\"].sum()))\n",
    "print(\"Still missing total:\", int(diag_df[\"still_missing_after\"].sum()))\n",
    "print(\"Top methods:\")\n",
    "print(summary_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3743025f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
